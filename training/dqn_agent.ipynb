{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Activation, Flatten\n",
    "from keras.optimizers import adam_v2\n",
    "from keras.callbacks import TensorBoard\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gym\n",
    "import gym_battleship\n",
    "\n",
    "if len(tf.config.list_physical_devices('GPU')) == 0:\n",
    "    raise Exception('No GPU found')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "DISCOUNT = 0.99\n",
    "REPLAY_MEMORY_SIZE = 50_000  # How many last steps to keep for model training\n",
    "MIN_REPLAY_MEMORY_SIZE = 2_000  # Minimum number of steps in a memory to start training\n",
    "MINIBATCH_SIZE = 64  # How many steps (samples) to use for training\n",
    "UPDATE_TARGET_EVERY = 5  # Terminal states (end of episodes)\n",
    "MODEL_NAME = '5x5-1-ship'\n",
    "MIN_REWARD = 20  # For model save\n",
    "MEMORY_FRACTION = 0.20\n",
    "EPISODES = 20_000\n",
    "\n",
    "# Environment settings\n",
    "ship_sizes = {3: 1}\n",
    "board_size = (5, 5)\n",
    "episode_steps = 100\n",
    "# TODO should configure this by board  size later\n",
    "reward_dictionary = {\n",
    "    'win': 100,  # for sinking all ships\n",
    "    'missed': -0.25,  # for missing a shot\n",
    "    'hit': 1,  # for hitting a ship\n",
    "    'repeat_missed': -5,  # for shooting at an already missed cell\n",
    "    'repeat_hit': -5  # for shooting at an already hit cell\n",
    "}\n",
    "\n",
    "# Exploration settings\n",
    "epsilon = 1\n",
    "EPSILON_DECAY = 0.99975\n",
    "MIN_EPSILON = 0.001\n",
    "\n",
    "#  Stats settings\n",
    "AGGREGATE_STATS_EVERY = 20  # episodes\n",
    "SHOW_PREVIEW = False\n",
    "\n",
    "# For stats\n",
    "ep_rewards = [-200]\n",
    "\n",
    "# For more repetitive results\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "# TODO can probably remove\n",
    "# Memory fraction, used mostly when training multiple agents\n",
    "#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=MEMORY_FRACTION)\n",
    "#backend.set_session(tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)))\n",
    "\n",
    "# Create models folder\n",
    "if not os.path.isdir('models'):\n",
    "    os.makedirs('models')\n",
    "\n",
    "env = gym.make('Battleship-v0', ship_sizes=ship_sizes, board_size=board_size, episode_steps=episode_steps,\n",
    "               reward_dictionary=reward_dictionary)\n",
    "\n",
    "\n",
    "# Own Tensorboard class\n",
    "class ModifiedTensorBoard(TensorBoard):\n",
    "\n",
    "    # Overriding init to set initial step and writer (we want one log file for all .fit() calls)\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.step = 1\n",
    "        self.writer = tf.summary.create_file_writer(self.log_dir)\n",
    "\n",
    "    # Overriding this method to stop creating default log writer\n",
    "    def set_model(self, model):\n",
    "        pass\n",
    "\n",
    "    # Overriden, saves logs with our step number\n",
    "    # (otherwise every .fit() will start writing from 0th step)\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.update_stats(**logs)\n",
    "\n",
    "    # Overriden\n",
    "    # We train for one batch only, no need to save anything at epoch end\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        pass\n",
    "\n",
    "    # Overriden, so won't close writer\n",
    "    def on_train_end(self, _):\n",
    "        pass\n",
    "\n",
    "    # Custom method for saving own metrics\n",
    "    # Creates writer, writes custom metrics and closes writer\n",
    "    # def update_stats(self, **stats):\n",
    "    #     self.writer.(stats, self.step)\n",
    "\n",
    "\n",
    "# Agent class\n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "\n",
    "        # Main model\n",
    "        self.model = self.create_model()\n",
    "\n",
    "        # Target network\n",
    "        self.target_model = self.create_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        print(self.model.summary())\n",
    "\n",
    "        # An array with last n steps for training\n",
    "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
    "\n",
    "\n",
    "        log_dir = \"logs/{}-{}\".format(MODEL_NAME, int(time.time()*1000))\n",
    "        # Custom tensorboard object\n",
    "        self.tensorboard = tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=log_dir)\n",
    "        self.file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "        self.file_writer.set_as_default()\n",
    "\n",
    "        #ModifiedTensorBoard(log_dir=\"logs/{}-{}\".format(MODEL_NAME, int(time.time())))\n",
    "\n",
    "        # Used to count when to update target network with main network's weights\n",
    "        self.target_update_counter = 0\n",
    "\n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "\n",
    "        # model.add(Conv2D(256, (3, 3),\n",
    "        #                  input_shape=env.observation_space.shape))  # Should be equal to the board size\n",
    "        # model.add(Activation('relu'))\n",
    "        # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        # model.add(Dropout(0.2))\n",
    "        #\n",
    "        # model.add(Conv2D(256, (3, 3)))\n",
    "        # model.add(Activation('relu'))\n",
    "        # model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        # model.add(Dropout(0.2))\n",
    "        #\n",
    "        # model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "        model.add(Flatten(input_shape=env.observation_space.shape))\n",
    "        model.add(Dense(32))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(32))\n",
    "        model.add(Activation('relu'))\n",
    "        # model.add(Dense(32))\n",
    "        # model.add(Activation('relu'))\n",
    "\n",
    "        model.add(Dense(env.action_space.n, activation='linear'))  # ACTION_SPACE_SIZE = how many choices (9)\n",
    "\n",
    "        model.compile(loss=\"mse\", optimizer=tf.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    # Adds step's data to a memory replay array\n",
    "    # (observation space, action, reward, new observation space, done)\n",
    "    def update_replay_memory(self, transition):\n",
    "        self.replay_memory.append(transition)\n",
    "\n",
    "    # Trains main network every step during episode\n",
    "    def train(self, terminal_state, step):\n",
    "\n",
    "        # Start training only if certain number of samples is already saved\n",
    "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
    "            return\n",
    "\n",
    "        # Get a minibatch of random samples from memory replay table\n",
    "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
    "\n",
    "        # Get current states from minibatch, then query NN model for Q values\n",
    "        current_states = np.array([transition[0] for transition in minibatch]) / 3\n",
    "        current_qs_list = self.model.predict(current_states)\n",
    "\n",
    "        # Get future states from minibatch, then query NN model for Q values\n",
    "        # When using target network, query it, otherwise main network should be queried\n",
    "        new_current_states = np.array([transition[3] for transition in minibatch]) / 3\n",
    "        future_qs_list = self.target_model.predict(new_current_states)\n",
    "\n",
    "        X = []\n",
    "        y = []\n",
    "\n",
    "        # Now we need to enumerate our batches\n",
    "        for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n",
    "\n",
    "            # If not a terminal state, get new q from future states, otherwise set it to 0\n",
    "            # almost like with Q Learning, but we use just part of equation here\n",
    "            if not done:\n",
    "                max_future_q = np.max(future_qs_list[index])\n",
    "                new_q = reward + DISCOUNT * max_future_q\n",
    "            else:\n",
    "                new_q = reward\n",
    "\n",
    "            # Update Q value for given state\n",
    "            current_qs = current_qs_list[index]\n",
    "            current_qs[action] = new_q\n",
    "\n",
    "            # And append to our training data\n",
    "            X.append(current_state)\n",
    "            y.append(current_qs)\n",
    "\n",
    "        # Fit on all samples as one batch, log only on terminal state\n",
    "        self.model.fit(np.array(X) / 3, np.array(y), batch_size=MINIBATCH_SIZE, verbose=0, shuffle=False)#,                       callbacks=[self.tensorboard] if terminal_state else None)\n",
    "\n",
    "        # Update target network counter every episode\n",
    "        if terminal_state:\n",
    "            self.target_update_counter += 1\n",
    "\n",
    "        # If counter reaches set value, update target network with weights of main network\n",
    "        if self.target_update_counter > UPDATE_TARGET_EVERY:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.target_update_counter = 0\n",
    "\n",
    "    # Queries main network for Q values given current observation space (environment state)\n",
    "    def get_qs(self, state):\n",
    "        return self.model.predict(np.array(state).reshape(-1, *state.shape) / 3)[0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_22 (Flatten)        (None, 25)                0         \n",
      "                                                                 \n",
      " dense_148 (Dense)           (None, 32)                832       \n",
      "                                                                 \n",
      " activation_108 (Activation)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_149 (Dense)           (None, 32)                1056      \n",
      "                                                                 \n",
      " activation_109 (Activation)  (None, 32)               0         \n",
      "                                                                 \n",
      " dense_150 (Dense)           (None, 25)                825       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,713\n",
      "Trainable params: 2,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|3         | 660/20000 [1:08:41<33:32:55,  6.24s/episodes]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agent = DQNAgent()\n",
    "\n",
    "# Iterate over episodes\n",
    "for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='episodes'):\n",
    "\n",
    "    # Update tensorboard step every episode\n",
    "    #agent.tensorboard.step = episode\n",
    "\n",
    "    # Restarting episode - reset episode reward and step number\n",
    "    episode_reward = 0\n",
    "    step = 1\n",
    "\n",
    "    # Reset environment and get initial state\n",
    "    current_state = env.reset()\n",
    "\n",
    "    # Reset flag and start iterating until episode ends\n",
    "    done = False\n",
    "    while not done:\n",
    "\n",
    "        # This part stays mostly the same, the change is to query a model for Q values\n",
    "        if np.random.random() > epsilon:\n",
    "            # Get action from Q table\n",
    "            action = int(np.argmax(agent.get_qs(current_state)))\n",
    "\n",
    "        else:\n",
    "            # Get random action\n",
    "            action = int(np.random.randint(0, env.action_space.n))\n",
    "\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Transform new continous state to new discrete state and count reward\n",
    "        episode_reward += reward\n",
    "\n",
    "        if SHOW_PREVIEW and not episode % AGGREGATE_STATS_EVERY:\n",
    "            env.render()\n",
    "\n",
    "        # Every step we update replay memory and train main network\n",
    "        agent.update_replay_memory((current_state, action, reward, new_state, done))\n",
    "        agent.train(done, step)\n",
    "\n",
    "        current_state = new_state\n",
    "        step += 1\n",
    "\n",
    "    # Append episode reward to a list and log stats (every given number of episodes)\n",
    "    ep_rewards.append(episode_reward)\n",
    "    if not episode % AGGREGATE_STATS_EVERY or episode == 1:\n",
    "        average_reward = sum(ep_rewards[-AGGREGATE_STATS_EVERY:]) / len(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        min_reward = min(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        max_reward = max(ep_rewards[-AGGREGATE_STATS_EVERY:])\n",
    "        tf.summary.scalar('reward_avg', data=average_reward, step=episode)\n",
    "        tf.summary.scalar('reward_min', data=min_reward, step=episode)\n",
    "\n",
    "\n",
    "        # Save model, but only when min reward is greater or equal a set value\n",
    "        if min_reward >= MIN_REWARD:\n",
    "            agent.model.save(\n",
    "                f'models/{MODEL_NAME}__{max_reward:_>7.2f}max_{average_reward:_>7.2f}avg_{min_reward:_>7.2f}min__{int(time.time())}.model')\n",
    "\n",
    "    # Decay epsilon\n",
    "    if epsilon > MIN_EPSILON:\n",
    "        epsilon *= EPSILON_DECAY\n",
    "        epsilon = max(MIN_EPSILON, epsilon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}